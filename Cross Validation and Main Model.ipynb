{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208e4efe",
   "metadata": {},
   "source": [
    "### Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91925a",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b16594d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Random Forest\n",
      "Confusion Matrix:\n",
      "[[12661  2668]\n",
      " [ 1340  2451]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86     15329\n",
      "         1.0       0.48      0.65      0.55      3791\n",
      "\n",
      "    accuracy                           0.79     19120\n",
      "   macro avg       0.69      0.74      0.71     19120\n",
      "weighted avg       0.82      0.79      0.80     19120\n",
      "\n",
      "Average Cross-Validation Score: 0.8230\n",
      "False Negatives: 1340\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Classifier: Gradient Boosting\n",
      "Confusion Matrix:\n",
      "[[9368 5961]\n",
      " [1237 2554]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.61      0.72     15329\n",
      "         1.0       0.30      0.67      0.42      3791\n",
      "\n",
      "    accuracy                           0.62     19120\n",
      "   macro avg       0.59      0.64      0.57     19120\n",
      "weighted avg       0.77      0.62      0.66     19120\n",
      "\n",
      "Average Cross-Validation Score: 0.6509\n",
      "False Negatives: 1237\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Classifier: Logistic Regression\n",
      "Confusion Matrix:\n",
      "[[10386  4943]\n",
      " [ 2185  1606]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.68      0.74     15329\n",
      "         1.0       0.25      0.42      0.31      3791\n",
      "\n",
      "    accuracy                           0.63     19120\n",
      "   macro avg       0.54      0.55      0.53     19120\n",
      "weighted avg       0.71      0.63      0.66     19120\n",
      "\n",
      "Average Cross-Validation Score: 0.5563\n",
      "False Negatives: 2185\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Classifier: Decision Tree\n",
      "Confusion Matrix:\n",
      "[[11961  3368]\n",
      " [ 1378  2413]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.78      0.83     15329\n",
      "         1.0       0.42      0.64      0.50      3791\n",
      "\n",
      "    accuracy                           0.75     19120\n",
      "   macro avg       0.66      0.71      0.67     19120\n",
      "weighted avg       0.80      0.75      0.77     19120\n",
      "\n",
      "Average Cross-Validation Score: 0.7801\n",
      "False Negatives: 1378\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Classifier: K-Nearest Neighbors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[11174  4155]\n",
      " [ 1464  2327]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.73      0.80     15329\n",
      "         1.0       0.36      0.61      0.45      3791\n",
      "\n",
      "    accuracy                           0.71     19120\n",
      "   macro avg       0.62      0.67      0.63     19120\n",
      "weighted avg       0.78      0.71      0.73     19120\n",
      "\n",
      "Average Cross-Validation Score: 0.7782\n",
      "False Negatives: 1464\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Classifier: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\fahad\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10859  4470]\n",
      " [  882  2909]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.71      0.80     15329\n",
      "         1.0       0.39      0.77      0.52      3791\n",
      "\n",
      "    accuracy                           0.72     19120\n",
      "   macro avg       0.66      0.74      0.66     19120\n",
      "weighted avg       0.82      0.72      0.75     19120\n",
      "\n",
      "Average Cross-Validation Score: 0.7538\n",
      "False Negatives: 882\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Read the dataset and preprocess it\n",
    "file_path = r'C:\\Users\\fahad\\Documents\\PostGrad\\CE888\\Stress_dataset\\Stress-Predict-Dataset-main\\participants\\combined_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(['Timestamp', 'participant_number'], axis=1)\n",
    "df = df.fillna(df.mean())\n",
    "df = df.groupby('stress_class').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = df.drop('stress_class', axis=1)\n",
    "y = df['stress_class']\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
    "]\n",
    "\n",
    "# Initialize StratifiedKFold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Train and evaluate the classifiers\n",
    "for name, classifier in classifiers:\n",
    "    print(f\"Classifier: {name}\")\n",
    "    classifier.fit(X_train_smote, y_train_smote)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(classifier, X_train_smote, y_train_smote, cv=cv, scoring='accuracy')\n",
    "    avg_score = scores.mean()\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Extract the false negatives from the confusion matrix\n",
    "    false_negatives = conf_mat[1][0]\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_mat)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    print(f\"Average Cross-Validation Score: {avg_score:.4f}\")\n",
    "    print(f\"False Negatives: {false_negatives}\")\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62046a",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f52674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.85\n",
      "Updated Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.77      0.84     15329\n",
      "         1.0       0.45      0.76      0.57      3791\n",
      "\n",
      "    accuracy                           0.77     19120\n",
      "   macro avg       0.69      0.77      0.70     19120\n",
      "weighted avg       0.83      0.77      0.79     19120\n",
      "\n",
      "Updated Confusion Matrix:\n",
      "[[11785  3544]\n",
      " [  896  2895]]\n",
      "Accuracy: 0.77\n",
      "Precision: 0.83\n",
      "Recall: 0.77\n",
      "F1 Score: 0.79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Set the file path\n",
    "file_path = r'C:\\Users\\fahad\\Documents\\PostGrad\\CE888\\Stress_dataset\\Stress-Predict-Dataset-main\\participants\\combined_data.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop Timestamp and participant_number columns as they're not useful for prediction\n",
    "df = df.drop(['Timestamp', 'participant_number'], axis=1)\n",
    "\n",
    "# Handle missing values by filling them with the mean value of the respective column\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Remove classes with only one sample\n",
    "df = df.groupby('stress_class').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = df.drop('stress_class', axis=1)\n",
    "y = df['stress_class']\n",
    "\n",
    "# Perform feature scaling to standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform feature selection using ANOVA F-value (f_classif) and keep all features for now\n",
    "selector = SelectKBest(score_func=f_classif, k='all')\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Split the data into training and test sets using stratified sampling to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Oversample the minority class using SMOTE to balance the class distribution\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Set the best hyperparameters for the Random Forest classifier\n",
    "best_params_rf = {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 40}\n",
    "\n",
    "# Create the Random Forest Classifier with the best hyperparameters\n",
    "rf_clf = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "\n",
    "# Set the best hyperparameters for the XGBoost classifier\n",
    "best_params_xgb = {'max_depth': 5, 'n_estimators': 100, 'learning_rate': 0.1}\n",
    "\n",
    "# Create the XGBoost Classifier with the best hyperparameters\n",
    "xgb_clf = XGBClassifier(**best_params_xgb, random_state=42)\n",
    "\n",
    "# Create a voting classifier with the two classifiers, using soft voting to average probabilities\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rf_clf), ('xgb', xgb_clf)], voting='soft')\n",
    "\n",
    "# Train the voting classifier on the oversampled training data\n",
    "voting_clf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "#In the last part of the code, we are calculating the ROC curve and AUC score to evaluate the performance of the classifier. \n",
    "#The AUC score represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. A higher AUC score indicates a better classifier.\n",
    "#We then find the optimal threshold that maximizes the difference between\n",
    "#the True Positive Rate (TPR) and the False Positive Rate (FPR). \n",
    "#This threshold helps us balance the trade-off between TPR and FPR. By updating the predicted classes \n",
    "#based on the optimal threshold, we can improve the performance of the classifier.\n",
    "#Finally, we print the updated classification report, confusion matrix, and various performance metrics\n",
    "#like accuracy, precision, recall, and F1 score to assess the model's performance after adjusting the threshold.\n",
    "\n",
    "# Calculate the predicted probabilities for the test set\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC curve and AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "#Find the optimal threshold by maximizing the difference between TPR (True Positive Rate) and FPR (False Positive Rate)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "#Update the predicted classes based on the optimal threshold\n",
    "y_pred = (y_pred_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "#Print the updated classification report\n",
    "print(\"Updated Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Calculate and print the updated confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(\"Updated Confusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "#calculate and print various performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f5708f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
